{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314015a1",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Assuming we have a line represented by the equation \n",
    "\n",
    "$$y = w_1x + w_2$$\n",
    "\n",
    "and a point with coordinates $(p,q)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd5255",
   "metadata": {},
   "source": [
    "### Absolute Trick\n",
    "\n",
    "We try to move the line closer to the point\n",
    "\n",
    "$$$y = (w_1 + p)x + (w_2 + 1)$$\n",
    "\n",
    "This ends up being too large of a step, and over-corrected our line.\n",
    "\n",
    "Instead , we utilize a small number called a learning rate, referred to as alpha($\\alpha$)\n",
    "\n",
    "$$y = (w_1 + p\\alpha)x + (w_2 + \\alpha)$$\n",
    "\n",
    "above works when the point(p, q) is above the line.\n",
    "\n",
    "When the point is udnerneath the line we need to substract in order to move the line appropriately\n",
    "\n",
    "$$y = (w_1 - p\\alpha)x + (w_2 - \\alpha)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d1bf9f",
   "metadata": {},
   "source": [
    "### Square Trick\n",
    "If we have a point that is close to a line, we want to move the line very little.\n",
    "\n",
    "If we have a point is far from the line, we want to move the line a lot more.\n",
    "\n",
    "The Absolute Trick dose not take into account how far the point is from the line.\n",
    "\n",
    "The square trick addresses this.\n",
    "\n",
    "The point over the line has coordinates $(p,q)$ , and the corresponding point on the line is $(p,q^′)$ . The vertical distance between the point and the line is $(q - q^′)$\n",
    "\n",
    "$$y = (w_1 + p(q - q^′)\\alpha)x + (w_2 + (q - q^′)\\alpha)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0cc402",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "We move the line, and calculate the error.\n",
    "- we move in this direction and see that the error increases, so that's not the way to go\n",
    "- we move in the other direction and see that the error decreased, so we pick this one\n",
    "\n",
    "repeat the steps many times over and over. every time descend the error a bit until we get the perfect line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723fe34e",
   "metadata": {},
   "source": [
    "**The two most common error functions for linear regression**\n",
    "\n",
    "- Mean absolute error\n",
    "- Mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47387c14",
   "metadata": {},
   "source": [
    "### Mean absolute error\n",
    "We have some points with coordinates $(x_1,y_1),(x_2,y_2)...(x_m,y_m)$, and a line is called $\\hat{Y}$.\n",
    "The corresponding point on the line are $(x_1,\\hat{y}_1),(x_2,\\hat{y}_2)...(x_m,\\hat{y}_m)$.\n",
    "\n",
    "The vertical distance from the point to the line is $(y - \\hat{y})$, we call it the error.\n",
    "\n",
    "so the total error is the sum of all these distance for all the points\n",
    "\n",
    "$$Error = \\sum_{i=1}^m | y - \\hat{y}|$$\n",
    "\n",
    "the mean absolute error is \n",
    "\n",
    "$$Error = \\frac{1}{m} \\sum_{i=1}^m | y - \\hat{y}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a606a",
   "metadata": {},
   "source": [
    "### Mean squared error\n",
    "Similar to the mean absolute Error,bu instead of taking the distance between the point and the prediction, we draw a square with this segment as it's side.\n",
    "\n",
    "$$Error = \\frac{1}{2m} \\sum_{i=1}^m ( y - \\hat{y})^2$$\n",
    "\n",
    "the one half is going to be there for convenience becuase later we will take the derivative of this error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c685702",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent vs Stochastic Gradient Descent\n",
    "\n",
    "we have two ways to do linear regression.\n",
    "\n",
    "- by applying the squared(or absolute) trick at every point one by one, and repeating this process many times\n",
    "- by applying the squared(or absolute) trick at every point all at the same time, and repeating this process many times\n",
    "\n",
    "The former one is called ***stochastic gradient descent***. The latter is called ***batch gradient descent***.\n",
    "\n",
    "In most cases, the best way is to split your data into many small batches. Each batch, with roughly the same number of point. Then use each batch to update your weight.\n",
    "This is call ***mini-batch gradient descent***.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e28820",
   "metadata": {},
   "source": [
    "# The Same thing\n",
    "\n",
    "Actually, the Absolute/Square Trick and the Gradient Descent are doing the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374629f9",
   "metadata": {},
   "source": [
    "### The squared trick and The mean squared error\n",
    "\n",
    "We have the points $(x, y)$ and the  equation\n",
    "\n",
    "$$\\hat{y} = w_1x + w_2$$\n",
    "\n",
    "We want to make sure the direction we moved to reduce the mean squared error. \n",
    "\n",
    "What's the minimum value of mean squared error ?\n",
    "\n",
    "$$E = \\frac{1}{2}  ( y - \\hat{y})^2$$\n",
    "\n",
    "In order to minimize it, let's take the derivatives with respect to $w_1$ and $w_2$.\n",
    "\n",
    "Let's do it to $w_1$ first.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E}{\\partial w_1}\n",
    "&= \\frac{\\partial E}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_1} \\\\\n",
    "&= \\frac{\\partial {(\\frac{1}{2}  ( y - \\hat{y})^2)}}{\\partial \\hat{y}} \\frac{\\partial (w_1x + w_2)}{\\partial w_1} \\\\\n",
    "&= \\frac{\\partial {(\\frac{1}{2}  ( y^2 - 2y\\hat{y} +(\\hat{y})^2))}}{\\partial \\hat{y}} x \\\\\n",
    "&= \\frac{\\partial {(\\frac{y^2}{2} - \\frac{2y\\hat{y}}{2} +\\frac{(\\hat{y})^2}{2})}}{\\partial \\hat{y}} x \\\\\n",
    "&= (0 - y + \\hat{y}) x \\\\\n",
    "&= -(y - \\hat{y}) x\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa8d8e",
   "metadata": {},
   "source": [
    "If we think about  the point $(p, q)$, we got the first part of Square Trick\n",
    "\n",
    "$$ -p(q - q^′)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc50175",
   "metadata": {},
   "source": [
    "Now, let's take the derivatives with respect to $w_2$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E}{\\partial w_2}\n",
    "&= \\frac{\\partial E}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_2} \\\\\n",
    "&= \\frac{\\partial {(\\frac{1}{2}  ( y - \\hat{y})^2)}}{\\partial \\hat{y}} \\frac{\\partial (w_1x + w_2)}{\\partial w_2} \\\\\n",
    "&= \\frac{\\partial {(\\frac{1}{2}  ( y^2 - 2y\\hat{y} +(\\hat{y})^2))}}{\\partial \\hat{y}} 1 \\\\\n",
    "&= \\frac{\\partial {(\\frac{y^2}{2} - \\frac{2y\\hat{y}}{2} +\\frac{(\\hat{y})^2}{2})}}{\\partial \\hat{y}} \\\\\n",
    "&= (0 - y + \\hat{y}) \\\\\n",
    "&= -(y - \\hat{y})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4540fd",
   "metadata": {},
   "source": [
    "So, we got the second part\n",
    "$$ -(q - q^′)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850839aa",
   "metadata": {},
   "source": [
    "Let's look back to the Square Trick\n",
    "\n",
    "$$y = (w_1 + p(q - q^′)\\alpha)x + (w_2 + (q - q^′)\\alpha)$$\n",
    "\n",
    "See, they are the same thing. ( Almost the same thing. As you see, we need to flip the sign . Why ? it's another story.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "303fa613b6f3e1efefe7bb28036e305e1021fa6bdb083a5f9fd57f9d9bbad8eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
