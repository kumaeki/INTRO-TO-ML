{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dec9e40",
   "metadata": {},
   "source": [
    "# Bias and Variance\n",
    "this [Wikipedia article on the bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) discusses the central problem in supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57570003",
   "metadata": {},
   "source": [
    "# Ensemble methods in the scikit-learn library:\n",
    "- [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)\n",
    "Discusses how the Bagging classifier is used to fit base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.\n",
    "\n",
    "- [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "Discusses how the RandomForest classifier fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "- [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)\n",
    "Discusses how the AdaBoost classifier fits on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "\n",
    "Another really useful guide for ensemble methods, which can also all be extended to regression problems, can be found in the [documentation](https://scikit-learn.org/stable/modules/ensemble.html) here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e429ef",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "- [The original paper](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf) - A link to the original paper on boosting by Yoav Freund and Robert E. Schapire.\n",
    "- [An explanation about why boosting is so important](https://medium.com/kaggle-blog) - A great article on boosting by a Kaggle master, Ben Gorman.\n",
    "- [A useful Quora post](https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting) - A number of useful explanations about boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336f892",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "- Here is the [original paper](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf) from Freund and Schapire that is a short overview paper introducing the boosting algorithm AdaBoost, and explains the underlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boostingâ€™s the relationship to support-vector machines.\n",
    "- [A follow-up paper](https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf) from the same authors regarding several experiments with Adaboost.\n",
    "- [A great tutorial](http://rob.schapire.net/papers/explaining-adaboost.pdf) by Schapire explaining the many perspectives and analyses of AdaBoost that have been applied to explain or understand it as a learning method, with comparisons of both the strengths and weaknesses of the various approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
